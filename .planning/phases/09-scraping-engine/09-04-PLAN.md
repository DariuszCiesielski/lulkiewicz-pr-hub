---
phase: 09-scraping-engine
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - src/app/api/fb/scrape/check-cookies/route.ts
  - src/hooks/useScrapeJob.ts
  - src/components/fb/ScrapeProgress.tsx
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Przed scrapowaniem hook wywoluje POST /api/fb/scrape/check-cookies — ScrapeProgress wyswietla status 'Sprawdzanie cookies...'"
    - "Jesli health check zwroci 0 wynikow, uzytkownik widzi zolty alert z sugestia COOKIES_EXPIRED i przyciski Kontynuuj/Anuluj"
    - "Jesli health check zakoncz sie sukcesem (>0 wynikow), scrapowanie startuje automatycznie bez dodatkowej interakcji"
    - "Istniejacy flow scrapowania (single + bulk) dziala bez regresji"
  artifacts:
    - path: "src/app/api/fb/scrape/check-cookies/route.ts"
      provides: "Cookie health check endpoint — lightweight Apify Actor run"
      exports: ["POST"]
    - path: "src/hooks/useScrapeJob.ts"
      provides: "Status cookie_check ustawiany przed scrapowaniem"
      contains: "cookie_check"
    - path: "src/components/fb/ScrapeProgress.tsx"
      provides: "Renderowanie stanu cookie_check"
      contains: "Sprawdzanie cookies"
  key_links:
    - from: "src/hooks/useScrapeJob.ts"
      to: "/api/fb/scrape/check-cookies"
      via: "fetch POST before starting actual scrape"
      pattern: "fetch.*check-cookies"
    - from: "src/components/fb/ScrapeProgress.tsx"
      to: "ScrapeUIStatus"
      via: "cookie_check status rendering"
      pattern: "cookie_check"
---

<objective>
Implement pre-scrape cookie health check that validates FB cookies before starting actual Apify scraping.

Purpose: Close the last gap in Phase 9 — Truth #5 requires a pre-scrape cookie health check (testowy scrape maxPosts:1) instead of only post-scrape warning when 0 posts.

Output:
- New API route `POST /api/fb/scrape/check-cookies` — starts minimal Apify run, polls to completion, returns result
- Updated `useScrapeJob` hook — calls check-cookies before starting actual scrape, sets `status=cookie_check`
- Updated `ScrapeProgress` — renders cookie_check state and warning if check fails
</objective>

<execution_context>
@C:\Users\dariu\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\dariu\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-scraping-engine/09-VERIFICATION.md
@.planning/phases/09-scraping-engine/09-03-SUMMARY.md

# Key source files to reference
@src/lib/fb/apify-client.ts (startActorRun, getRunStatus, getDatasetItems, formatApifyDate)
@src/hooks/useScrapeJob.ts (hook to modify — add cookie check before scrape)
@src/components/fb/ScrapeProgress.tsx (component to modify — render cookie_check status)
@src/app/api/fb/scrape/route.ts (existing start endpoint — reference for pre-flight pattern)
@src/app/api/fb/scrape/process/route.ts (reference for loadScrapeConfig pattern, lines 118-135)
@src/types/fb.ts (ScrapeUIStatus already has 'cookie_check', SCRAPE_ERROR_MESSAGES has 'COOKIES_EXPIRED')
</context>

<tasks>

<task type="auto">
  <name>Task 1: Cookie health check API route</name>
  <files>src/app/api/fb/scrape/check-cookies/route.ts</files>
  <action>
    Create `POST /api/fb/scrape/check-cookies` endpoint that performs a lightweight Apify Actor run to validate cookies.

    **Request body:** `{ groupId: string }`

    **Flow:**
    1. `verifyAdmin()` + `getAdminClient()` (import from `@/lib/api/admin`)
    2. Load group (same as scrape/route.ts: id, facebook_url, cookies_encrypted, apify_actor_id)
    3. Load config: reuse the `loadScrapeConfig` pattern from process/route.ts — decrypt token from fb_settings, decrypt cookies (per-group or global), get actorId
       - IMPORTANT: Since `loadScrapeConfig` is a private function inside process/route.ts, extract the needed logic inline. DO NOT modify process/route.ts.
       - Token: query `fb_settings` for key='apify_token', decrypt value_encrypted
       - Cookies: if group.cookies_encrypted use that, else query fb_settings for key='fb_cookies', decrypt
       - ActorId: group.apify_actor_id or DEFAULT_ACTOR_ID ('curious_coder/facebook-post-scraper')
    4. Build minimal ApifyActorInput:
       ```ts
       const input: ApifyActorInput = {
         cookie: cookies,
         'scrapeGroupPosts.groupUrl': groupUrl,
         scrapeUntil: formatApifyDate(new Date()), // today — minimal window
         sortType: 'new_posts',
         minDelay: 1,
         maxDelay: 2,
         proxy: { useApifyProxy: true },
       };
       ```
       Note: There's no `maxPosts` param in ApifyActorInput. The minimal approach is using `scrapeUntil: today` which limits to posts from today only. This is the lightest possible run.
    5. Call `startActorRun(token, actorId, input)` — get runId + datasetId
    6. **Poll until completion** — loop with 3s sleep, max 90s timeout:
       ```ts
       const MAX_WAIT_MS = 90_000;
       const POLL_MS = 3_000;
       const start = Date.now();
       let runStatus;

       while (Date.now() - start < MAX_WAIT_MS) {
         runStatus = await getRunStatus(token, runId);
         const action = mapApifyStatusToAction(runStatus.status);
         if (action === 'fetch_results') break;
         if (action === 'mark_failed') {
           return NextResponse.json({ success: false, postsFound: 0, error: 'Actor run failed' });
         }
         await new Promise(r => setTimeout(r, POLL_MS));
       }
       ```
    7. If SUCCEEDED: call `getDatasetItems(token, datasetId, 0, 1)` — get total count from `total` field
    8. Return: `{ success: true, postsFound: total }` or `{ success: false, postsFound: 0, error: '...' }`

    **Error handling:**
    - If token/cookies missing: return 400 with SCRAPE_ERROR_MESSAGES['NO_TOKEN'] or ['NO_COOKIES']
    - If Apify call fails: return `{ success: false, postsFound: 0, error: message }`
    - If timeout (90s exceeded): return `{ success: false, postsFound: 0, error: 'Health check timeout' }`

    **Config:**
    - `export const maxDuration = 60;` (Vercel — actual timeout loop is 90s but Vercel will kill at 60s, account for that by using 50s in actual code)
    - Actually, use 45s for MAX_WAIT_MS to stay safely under Vercel's 60s limit with buffer for the rest of the function's execution time
  </action>
  <verify>
    - `npx tsc --noEmit` passes without errors
    - File exists at `src/app/api/fb/scrape/check-cookies/route.ts`
    - Exports `POST` function
    - Uses verifyAdmin() + getAdminClient()
    - Imports from `@/lib/fb/apify-client` (startActorRun, getRunStatus, getDatasetItems, mapApifyStatusToAction, formatApifyDate)
    - Returns `{ success, postsFound }` shape
  </verify>
  <done>
    POST /api/fb/scrape/check-cookies endpoint exists, validates cookies by starting minimal Apify run, polls to completion, returns postsFound count. Handles missing config (400), Apify errors, and timeout.
  </done>
</task>

<task type="auto">
  <name>Task 2: Hook cookie_check status + ScrapeProgress rendering</name>
  <files>
    src/hooks/useScrapeJob.ts
    src/components/fb/ScrapeProgress.tsx
  </files>
  <action>
    **Part A: Update useScrapeJob.ts**

    Add cookie health check before starting actual scrape. Modify both `startScrape` and the inline bulk-scrape logic.

    1. Add a new ref `skipCookieCheckRef = useRef(false)` — used when user clicks "Kontynuuj mimo to" after failed check
    2. Add `cookieCheckWarning` state: `useState<string | null>(null)` — warning message from failed check
    3. Add `proceedAfterWarning` callback — sets `skipCookieCheckRef.current = true`, resets warning, restarts the scrape
    4. Add `cookieCheckGroupRef = useRef<{id: string, name?: string} | null>(null)` — remembers which group to retry

    **Cookie check flow in `startScrape`:**
    ```
    Before the existing fetch('/api/fb/scrape', ...) call:

    a) If !skipCookieCheckRef.current:
       - setStatus('cookie_check')
       - setCookieCheckWarning(null)
       - fetch('/api/fb/scrape/check-cookies', { method: 'POST', body: JSON.stringify({ groupId }) })
       - If !res.ok or data.success === false:
         - If specifically config error (400): propagate to normal error state
         - If check returned { success: false } or postsFound === 0:
           - setCookieCheckWarning(SCRAPE_ERROR_MESSAGES['COOKIES_EXPIRED'].suggestion)
           - cookieCheckGroupRef.current = { id: groupId, name: groupName }
           - return (don't proceed to actual scrape)
       - If data.success && data.postsFound > 0: continue to actual scrape (fall through)
    b) Reset skipCookieCheckRef.current = false after use
    ```

    **For bulk scrape:** Apply the SAME cookie check logic inside the inline async block for each group. If a group's check fails, set warning and return from the current group (the bulk loop continues to the next group or pauses for user action).
    SIMPLIFICATION: For bulk scrape, SKIP cookie check — it would be too disruptive to pause bulk for each group. The check is meaningful for single-group scrape only. Add a comment explaining this.

    5. Update `UseScrapeJobReturn` interface to include:
       - `cookieCheckWarning: string | null`
       - `proceedAfterWarning: () => void`

    6. Update `reset` to also clear `cookieCheckWarning` and `skipCookieCheckRef`

    **Part B: Update ScrapeProgress.tsx**

    Add rendering for `cookie_check` status and cookie warning state.

    1. Add `cookieCheckWarning?: string | null` and `onProceedAnyway?: () => void` to `ScrapeProgressProps`

    2. Add cookie_check status rendering AFTER the `isWaitingBetweenGroups` check and BEFORE `starting`:
    ```tsx
    // Cookie health check
    if (status === 'cookie_check') {
      return (
        <div className="sticky top-0 z-10 rounded-lg border p-4 mb-4" style={{ borderColor: 'var(--border-primary)', backgroundColor: 'var(--bg-secondary)' }}>
          <div className="flex items-center gap-3">
            <Loader2 className="h-5 w-5 animate-spin flex-shrink-0" style={{ color: '#eab308' }} />
            <p className="text-sm font-medium" style={{ color: 'var(--text-primary)' }}>
              Sprawdzanie cookies Facebook...
            </p>
          </div>
        </div>
      );
    }
    ```

    3. Add cookie warning rendering — show when `cookieCheckWarning` is truthy AND status is NOT actively scraping:
    Place this BEFORE the `starting` check (after cookie_check):
    ```tsx
    if (cookieCheckWarning && status === 'idle') {
      return (
        <div className="sticky top-0 z-10 rounded-lg border p-4 mb-4"
          style={{ borderColor: 'rgba(234, 179, 8, 0.3)', backgroundColor: 'rgba(234, 179, 8, 0.05)' }}>
          <div className="flex items-start gap-3">
            <AlertTriangle className="h-5 w-5 flex-shrink-0 mt-0.5" style={{ color: '#eab308' }} />
            <div className="flex-1 min-w-0">
              <p className="text-sm font-medium" style={{ color: '#eab308' }}>
                Sprawdzanie cookies nie powiodlo sie
              </p>
              <p className="text-xs mt-1" style={{ color: 'var(--text-muted)' }}>
                {cookieCheckWarning}
              </p>
              <div className="flex items-center gap-2 mt-2">
                {onProceedAnyway && (
                  <button onClick={onProceedAnyway}
                    className="rounded-md px-3 py-1 text-xs font-medium text-white transition-colors hover:opacity-90"
                    style={{ backgroundColor: '#eab308' }}>
                    Kontynuuj mimo to
                  </button>
                )}
                <button onClick={onReset}
                  className="rounded-md border px-3 py-1 text-xs font-medium transition-colors hover:opacity-80"
                  style={{ borderColor: 'var(--border-primary)', color: 'var(--text-secondary)' }}>
                  Anuluj
                </button>
              </div>
            </div>
          </div>
        </div>
      );
    }
    ```

    4. Update the groups/page.tsx to pass new props to ScrapeProgress:
    ```tsx
    <ScrapeProgress
      status={scrapeStatus}
      progress={scrapeProgress}
      error={scrapeError}
      cookieCheckWarning={cookieCheckWarning}
      onProceedAnyway={proceedAfterWarning}
      onRetry={...}
      onReset={resetScrape}
    />
    ```
    And destructure `cookieCheckWarning` and `proceedAfterWarning` from `useScrapeJob()`.

    **Important constraints:**
    - Do NOT change the ScrapeUIStatus type — 'cookie_check' is ALREADY there
    - Do NOT modify process/route.ts or scrape/route.ts
    - Keep the existing null-return for `status === 'idle' && !progress.isWaitingBetweenGroups` — the warning intercepts BEFORE this check (when cookieCheckWarning is set)
    - The `status === 'idle'` check for warning display works because after a failed cookie check, the hook sets status back to idle and sets the warning message
  </action>
  <verify>
    - `npx tsc --noEmit` passes without errors
    - `useScrapeJob.ts`: grep for 'cookie_check' confirms status is set
    - `useScrapeJob.ts`: grep for 'check-cookies' confirms fetch call exists
    - `ScrapeProgress.tsx`: grep for 'Sprawdzanie cookies' confirms rendering exists
    - `ScrapeProgress.tsx`: grep for 'Kontynuuj mimo to' confirms proceed button exists
    - `ScrapeProgress.tsx`: grep for 'cookieCheckWarning' confirms prop is handled
    - Destructuring in groups/page.tsx includes `cookieCheckWarning` and `proceedAfterWarning`
  </verify>
  <done>
    Hook sets status='cookie_check' before scraping, calls check-cookies endpoint, shows warning with Kontynuuj/Anuluj if check fails, proceeds normally if check passes. ScrapeProgress renders cookie_check spinner and failed-check warning. Groups page passes new props.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` — TypeScript compiles without errors
2. `src/app/api/fb/scrape/check-cookies/route.ts` exists and exports POST
3. `src/hooks/useScrapeJob.ts` sets status='cookie_check' before fetch to /api/fb/scrape
4. `src/components/fb/ScrapeProgress.tsx` renders spinner for cookie_check and warning for failed check
5. `ScrapeUIStatus` type in fb.ts still includes 'cookie_check' (no change needed, already present)
6. `SCRAPE_ERROR_MESSAGES['COOKIES_EXPIRED']` is used in the warning display
7. Existing scrape flow (single + bulk) works without regression
</verification>

<success_criteria>
- Pre-scrape cookie health check fully functional: hook calls check-cookies endpoint, waits for minimal Apify run, alerts if cookies expired
- ScrapeProgress shows "Sprawdzanie cookies Facebook..." during check
- Failed check shows yellow warning with "Kontynuuj mimo to" + "Anuluj" buttons
- Successful check proceeds to normal scrape transparently
- All TypeScript compiles, no regressions in existing flow
</success_criteria>

<output>
After completion, create `.planning/phases/09-scraping-engine/09-04-SUMMARY.md`
</output>
