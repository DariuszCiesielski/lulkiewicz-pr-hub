---
phase: 09-scraping-engine
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - src/app/api/fb/scrape/route.ts
  - src/app/api/fb/scrape/process/route.ts
  - src/app/api/fb/scrape/status/[jobId]/route.ts
autonomous: true

must_haves:
  truths:
    - "POST /api/fb/scrape tworzy fb_scrape_jobs row i zwraca jobId — blokuje duplikaty (409 jesli aktywny job istnieje)"
    - "POST /api/fb/scrape/process operuje w 3 trybach: (1) start Apify run, (2) poll status, (3) fetch dataset + upsert postow"
    - "Upsert postow dziala z ON CONFLICT (group_id, facebook_post_id) — aktualizuje istniejace posty zamiast tworzyc duplikaty"
    - "Bledy Apify (FAILED, TIMED-OUT, ABORTED) sa logowane w fb_scrape_jobs.error_message i zwracane do klienta"
    - "GET /api/fb/scrape/status/[jobId] zwraca aktualny status joba (do recovery/reconnect)"
  artifacts:
    - path: "src/app/api/fb/scrape/route.ts"
      provides: "POST endpoint tworzacy scrape job"
      exports: ["POST"]
    - path: "src/app/api/fb/scrape/process/route.ts"
      provides: "POST endpoint z 3-mode processing (start, poll, fetch+upsert)"
      exports: ["POST"]
    - path: "src/app/api/fb/scrape/status/[jobId]/route.ts"
      provides: "GET endpoint do odczytu statusu joba"
      exports: ["GET"]
  key_links:
    - from: "src/app/api/fb/scrape/process/route.ts"
      to: "src/lib/fb/apify-client.ts"
      via: "import startActorRun, getRunStatus, getDatasetItems"
      pattern: "import.*from.*@/lib/fb/apify-client"
    - from: "src/app/api/fb/scrape/process/route.ts"
      to: "src/lib/fb/post-mapper.ts"
      via: "import mapApifyPostToFbPost"
      pattern: "import.*from.*@/lib/fb/post-mapper"
    - from: "src/app/api/fb/scrape/process/route.ts"
      to: "src/lib/crypto/encrypt.ts"
      via: "decrypt Apify token + cookies"
      pattern: "import.*decrypt.*from.*@/lib/crypto/encrypt"
---

<objective>
3 API routes dla scraping engine: start job, process (3 modes), status read.

Purpose: Plan 02 implementuje server-side scraping pipeline. POST /api/fb/scrape tworzy job. POST /api/fb/scrape/process jest wolany cyklicznie przez hook kliencki i operuje w 3 trybach — startuje Apify run, odpytuje status, albo pobiera wyniki i upsertuje posty. GET /api/fb/scrape/status/[jobId] umozliwia recovery po utracie polaczenia.

Output: 3 pliki API routes, kompletny server-side scraping pipeline.
</objective>

<execution_context>
@C:\Users\dariu\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\dariu\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-scraping-engine/09-RESEARCH.md
@.planning/phases/09-scraping-engine/09-01-SUMMARY.md

@src/app/api/sync/route.ts
@src/app/api/sync/process/route.ts
@src/app/api/fb-settings/route.ts
@src/lib/fb/apify-client.ts
@src/lib/fb/post-mapper.ts
@src/lib/crypto/encrypt.ts
@src/lib/api/admin.ts
@src/types/fb.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Scrape start route + status route</name>
  <files>src/app/api/fb/scrape/route.ts, src/app/api/fb/scrape/status/[jobId]/route.ts</files>
  <action>
**1) Utworz `src/app/api/fb/scrape/route.ts`** — POST endpoint tworzacy scrape job.

Wzorzec: src/app/api/sync/route.ts (ale bez demo-scope, FB jest admin-only).

```
export const maxDuration = 30;

POST handler:
1. verifyAdmin() -> 403 jesli nie admin
2. Parse body: { groupId: string, scrapeUntilDays?: number }
   - scrapeUntilDays domyslnie 30 (ile dni wstecz scrapowac)
3. Walidacja: groupId wymagany (400)
4. Sprawdz grupe: adminClient.from('fb_groups').select('id, name, status, deleted_at').eq('id', groupId).is('deleted_at', null).single()
   - Jesli nie znaleziona lub status !== 'active' -> 404 "Grupa nie istnieje lub jest nieaktywna"
5. Sprawdz aktywne joby: adminClient.from('fb_scrape_jobs').select('id, status').eq('group_id', groupId).in('status', ['pending', 'running', 'downloading'])
   - Jesli length > 0 -> 409 "Scrapowanie juz trwa dla tej grupy"
6. Sprawdz konfiguracje (pre-flight): pobierz z fb_settings klucze apify_token i fb_cookies
   - Jesli brak apify_token -> 400 z SCRAPE_ERROR_MESSAGES['NO_TOKEN']
   - Jesli brak fb_cookies I brak group.cookies_encrypted -> 400 z SCRAPE_ERROR_MESSAGES['NO_COOKIES']
7. Utworz fb_scrape_jobs row: { group_id: groupId, status: 'pending', started_at: new Date().toISOString() }
   - .select('id, status').single()
8. Zwroc { jobId: job.id, status: 'pending', groupName: group.name }
```

Importy: verifyAdmin + getAdminClient z @/lib/api/admin, SCRAPE_ERROR_MESSAGES z @/types/fb.

**2) Utworz `src/app/api/fb/scrape/status/[jobId]/route.ts`** — GET endpoint do odczytu statusu.

```
export const maxDuration = 10;

GET handler (z params):
1. verifyAdmin() -> 403
2. Wyciagnij jobId z params (Next.js App Router dynamic route)
3. Pobierz job: adminClient.from('fb_scrape_jobs').select('id, group_id, status, posts_found, posts_new, posts_updated, apify_run_id, error_message, started_at, completed_at, created_at').eq('id', jobId).single()
   - 404 jesli nie znaleziony
4. Zwroc JSON z danymi joba
```

UWAGA na Next.js 15 App Router: params jest Promise, uzyj `const { jobId } = await params`.
  </action>
  <verify>
  - `npx tsc --noEmit` — brak bledow
  - Pliki istnieja: src/app/api/fb/scrape/route.ts, src/app/api/fb/scrape/status/[jobId]/route.ts
  - Grep: `verifyAdmin` importowane w obu plikach
  - Grep: `SCRAPE_ERROR_MESSAGES` uzyte w scrape/route.ts
  </verify>
  <done>POST /api/fb/scrape tworzy job z pre-flight validation (grupa aktywna, brak duplikatu, config OK). GET /api/fb/scrape/status/[jobId] zwraca status joba. Oba uzywaja verifyAdmin.</done>
</task>

<task type="auto">
  <name>Task 2: Scrape process route (3-mode pipeline)</name>
  <files>src/app/api/fb/scrape/process/route.ts</files>
  <action>
Utworz `src/app/api/fb/scrape/process/route.ts` — serce scraping pipeline. Ten endpoint jest wolany cyklicznie przez hook kliencki (co 5s) i operuje w 3 trybach.

Wzorzec: src/app/api/sync/process/route.ts (ale z Apify zamiast Graph API).

```
export const maxDuration = 60;
const SAFETY_TIMEOUT_MS = 50_000;
const DATASET_PAGE_SIZE = 200;
const UPSERT_BATCH_SIZE = 100;

POST handler:
1. verifyAdmin() -> 403
2. Parse body: { jobId: string }
3. Pobierz job: adminClient.from('fb_scrape_jobs').select('*').eq('id', jobId).single() -> 404
4. Walidacja status: allowedStatuses = ['pending', 'running', 'downloading']. Jesli job.status nie w tym zbiorze -> 400
5. Pobierz grupe: adminClient.from('fb_groups').select('id, facebook_url, cookies_encrypted, apify_actor_id').eq('id', job.group_id).single() -> 404 jesli nie znaleziona

--- MODE 1: Start Apify run (job.status === 'pending', brak apify_run_id) ---

6. Zaladuj config: loadScrapeConfig(adminClient, job.group_id)
   - Pobierz apify_token, fb_cookies, apify_actor_id z fb_settings
   - Decrypt token
   - Cookies: per-group override (group.cookies_encrypted) lub global (fb_cookies z fb_settings)
   - Decrypt cookies i JSON.parse
   - actorId: group.apify_actor_id || setting apify_actor_id || 'curious_coder/facebook-post-scraper'

7. Zbuduj actor input:
   ```
   const scrapeUntil = formatApifyDate(new Date(Date.now() - 30 * 24 * 60 * 60 * 1000)); // 30 dni wstecz
   const input: ApifyActorInput = {
     cookie: cookies,
     'scrapeGroupPosts.groupUrl': groupUrl,
     scrapeUntil,
     sortType: 'new_posts',
     minDelay: 3,
     maxDelay: 10,
     proxy: { useApifyProxy: true },
   };
   ```

8. Wywolaj startActorRun(token, actorId, input)
9. Zaktualizuj job: { status: 'running', apify_run_id: result.runId, metadata: { datasetId: result.datasetId } }
   - UWAGA: fb_scrape_jobs NIE MA kolumny metadata — zapisz datasetId w apify_run_id jako "{runId}|{datasetId}" i parsuj pozniej
   - Albo lepiej: zapisz datasetId w dodatkowym polu. Sprawdz schemat — jesli brak pola metadata, uzyj konwencji runId|datasetId w apify_run_id i split('|') przy odczycie.
   - DECYZJA: fb_scrape_jobs nie ma metadata. Uzyj pattern: apify_run_id = `{runId}` i pobierz datasetId z getRunStatus() response (ktory zawiera defaultDatasetId). To czystsze rozwiazanie.
10. Zwroc { status: 'running', mode: 'started', hasMore: true }

--- MODE 2: Poll Apify run status (job.status === 'running') ---

11. Wyciagnij runId z job.apify_run_id
12. Zaladuj token (decrypt z fb_settings, cache w scope requestu)
13. Wywolaj getRunStatus(token, runId)
14. Uzyj mapApifyStatusToAction(runStatus.status):
    - 'keep_polling': zwroc { status: 'running', mode: 'polling', apifyStatus: runStatus.status, statusMessage: runStatus.statusMessage, hasMore: true }
    - 'mark_failed': zaktualizuj job { status: 'failed', error_message: `Apify run zakonczony: ${runStatus.status} — ${runStatus.statusMessage || ''}`, completed_at: now() }. Zwroc { status: 'failed', error: SCRAPE_ERROR_MESSAGES[runStatus.status] || { message: runStatus.statusMessage, suggestion: 'Sprawdz logi Apify.' } }
    - 'fetch_results':
      a) Sprawdz statusMessage — jesli SUCCEEDED ale 'Finished scraping' nie jest w statusMessage, loguj warning
      b) Zaktualizuj job status na 'downloading'
      c) Zapisz datasetId: pobierz z runStatus.defaultDatasetId
      d) Zwroc { status: 'downloading', mode: 'ready_to_fetch', datasetId: runStatus.defaultDatasetId, hasMore: true }

--- MODE 3: Fetch dataset + upsert (job.status === 'downloading') ---

15. Zaladuj token (decrypt)
16. Pobierz datasetId — wywolaj getRunStatus(token, job.apify_run_id) i uzyj response.defaultDatasetId
17. Pobierz dataset items: getDatasetItems(token, datasetId, offset=job.posts_found, limit=DATASET_PAGE_SIZE)
    - UWAGA: posts_found sluzy jako offset do paginacji datasetu
18. Loguj sample: jesli job.posts_found === 0 (pierwszy batch), wywolaj logRawPostSample(items, 5)
19. Mapuj items: items.map(item => mapApifyPostToFbPost(item, job.group_id))
20. Upsert postow w batchach:
    - Batche po UPSERT_BATCH_SIZE (100)
    - Przed upsertem: pobierz istniejace facebook_post_id dla tej grupy (do zliczenia new vs updated)
    - adminClient.from('fb_posts').upsert(rows, { onConflict: 'group_id,facebook_post_id', ignoreDuplicates: false })
    - Zlicz postsNew i postsUpdated

21. Safety timeout check: jesli Date.now() - batchStartTime > SAFETY_TIMEOUT_MS, zapisz progress i zwroc hasMore: true

22. Zaktualizuj job: posts_found += items.length, posts_new += postsNew, posts_updated += postsUpdated

23. Sprawdz czy sa jeszcze dane: jesli items.total > (job.posts_found + items.length), zwroc { status: 'downloading', hasMore: true, postsFound, postsNew, postsUpdated, total: items.total }

24. Jesli wszystko pobrane (brak wiecej stron):
    - Zaktualizuj fb_groups: last_scraped_at = now(), total_posts = COUNT fb_posts dla tej grupy
    - Zaktualizuj job: status = 'completed', completed_at = now()
    - Zwroc { status: 'completed', postsFound: job.posts_found + items.length, postsNew: totalNew, postsUpdated: totalUpdated }

--- Error handling ---

- Wrap calosc w try/catch
- Na bledzie: zaktualizuj job { status: 'failed', error_message: msg, completed_at: now() }
- Zwroc 500 z { error: msg, status: 'failed' }

--- Helper: loadScrapeConfig ---

Wyodrebnij jako async function wewnatrz pliku (nie eksportuj):
```typescript
async function loadScrapeConfig(adminClient, groupId): Promise<{ token: string; cookies: ApifyCookieObject[]; actorId: string; groupUrl: string }>
```
Logika: jak w 09-RESEARCH.md Pattern 6 (pobierz z fb_settings, decrypt, per-group override cookies).

--- Helper: failJob ---

```typescript
async function failJob(adminClient, jobId, errorMessage): Promise<void>
```
Zaktualizuj job: { status: 'failed', error_message: errorMessage, completed_at: now() }.
  </action>
  <verify>
  - `npx tsc --noEmit` — brak bledow
  - Plik istnieje: src/app/api/fb/scrape/process/route.ts
  - Grep: `startActorRun` uzyte w MODE 1
  - Grep: `getRunStatus` uzyte w MODE 2
  - Grep: `getDatasetItems` uzyte w MODE 3
  - Grep: `mapApifyPostToFbPost` uzyte w MODE 3
  - Grep: `onConflict.*group_id.*facebook_post_id` — upsert z deduplikacja
  - Grep: `SAFETY_TIMEOUT_MS` — safety timeout zaimplementowany
  </verify>
  <done>POST /api/fb/scrape/process operuje w 3 trybach: MODE 1 startuje Apify run i zapisuje apify_run_id, MODE 2 polluje status i mapuje na akcje, MODE 3 pobiera dataset paginacyjnie i upsertuje posty z ON CONFLICT. Safety timeout 50s. Error handling z failJob helper. loadScrapeConfig pobiera i decryptuje token+cookies.</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` przechodzi
- 3 pliki API routes istnieja w src/app/api/fb/scrape/
- Kazdy route uzywa verifyAdmin() + getAdminClient()
- scrape/route.ts sprawdza aktywny job (409), sprawdza config (pre-flight)
- scrape/process/route.ts importuje z apify-client.ts i post-mapper.ts
- scrape/process/route.ts zawiera 3 mode'y (start, poll, fetch+upsert)
- Upsert uzywa onConflict: 'group_id,facebook_post_id'
- Status route obsluguje dynamic [jobId] parameter
</verification>

<success_criteria>
1. POST /api/fb/scrape tworzy job z walidacja (grupa aktywna, brak duplikatu, config dostepny)
2. POST /api/fb/scrape/process w MODE 1 startuje Apify Actor run
3. POST /api/fb/scrape/process w MODE 2 odpytuje status i mapuje na akcje
4. POST /api/fb/scrape/process w MODE 3 pobiera dataset i upsertuje posty z deduplikacja
5. GET /api/fb/scrape/status/[jobId] zwraca status joba
6. Bledy sa logowane w error_message i zwracane z polskim komunikatem
7. `npx tsc --noEmit` przechodzi
</success_criteria>

<output>
After completion, create `.planning/phases/09-scraping-engine/09-02-SUMMARY.md`
</output>
